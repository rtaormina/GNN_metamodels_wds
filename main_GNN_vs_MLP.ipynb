{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250b087c",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df603c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import os\n",
    "import sys\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from training.test import testing\n",
    "from training.train import training\n",
    "from utils.miscellaneous import create_folder_structure_MLPvsGNN\n",
    "from utils.miscellaneous import initalize_random_generators\n",
    "from utils.miscellaneous import read_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598b971",
   "metadata": {},
   "source": [
    "### Parse configuration file + initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0ef305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder: ./experiments/GNN_8000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# read config files\n",
    "cfg = read_config(\"configs/config_MLP_vs_GNN.yaml\")\n",
    "\n",
    "# create folder for results\n",
    "exp_name = cfg['exp_name']\n",
    "data_folder = cfg['data_folder']\n",
    "results_folder = create_folder_structure_MLPvsGNN(cfg, parent_folder='./experiments')\n",
    "\n",
    "all_wdn_names = cfg['networks']\n",
    "initalize_random_generators(cfg, count=0)\n",
    "\n",
    "# initialize pytorch device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device ='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee48c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = cfg['trainParams']['num_epochs']\n",
    "batch_size = cfg['trainParams']['batch_size']\n",
    "alpha = cfg['lossParams']['alpha']\n",
    "res_columns = ['train_loss', 'valid_loss','test_loss', 'r2_train', 'r2_valid',\n",
    "               'r2_test','total_params','total_time','num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87923cf",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6ee1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class PowerLogTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,log_transform=False,power=4,reverse=True):\n",
    "        if log_transform == True:\n",
    "            self.log_transform = log_transform\n",
    "            self.power = None\n",
    "        else:\n",
    "            self.power = power\n",
    "            self.log_transform = None\n",
    "        self.reverse=reverse\n",
    "        self.max_ = None\n",
    "        self.min_ = None\n",
    "        \n",
    "    def fit(self,X,y=None):        \n",
    "        self.max_ = np.max(X)\n",
    "        self.min_ = np.min(X)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if self.log_transform==True:\n",
    "            if self.reverse == True:\n",
    "                return np.log1p(self.max_-X)\n",
    "            else:\n",
    "                return np.log1p(X-self.min_)\n",
    "        else:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_-X)**(1/self.power )\n",
    "            else:\n",
    "                return (X-self.min_)**(1/self.power )\n",
    "            \n",
    "    def inverse_transform(self,X):\n",
    "        if self.log_transform==True:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_ - np.exp(X))\n",
    "            else:\n",
    "                return (np.exp(X) + self.min_)\n",
    "        else:\n",
    "            if self.reverse == True:\n",
    "                return (self.max_ - X**self.power )               \n",
    "            else:\n",
    "                return (X**self.power + self.min_)               \n",
    "    \n",
    "class GraphNormalizer:\n",
    "    def __init__(self, x_feat_names=['elevation','base_demand','base_head'],\n",
    "                 ea_feat_names=['diameter','length','roughness'], output='pressure'):        \n",
    "        # store \n",
    "        self.x_feat_names = x_feat_names\n",
    "        self.ea_feat_names = ea_feat_names\n",
    "        self.output = output\n",
    "        \n",
    "        # create separate scaler for each feature (can be improved, e.g., you can fit a scaler for multiple columns)\n",
    "        self.scalers = {}\n",
    "        for feat in self.x_feat_names:\n",
    "            if feat == 'elevation':\n",
    "                self.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "            else:\n",
    "                self.scalers[feat] = MinMaxScaler()\n",
    "        self.scalers[output] = PowerLogTransformer(log_transform=True,reverse=True)\n",
    "        for feat in self.ea_feat_names:\n",
    "            if feat == 'length':\n",
    "                self.scalers[feat] = PowerLogTransformer(log_transform=True,reverse=False)\n",
    "            else:\n",
    "                self.scalers[feat] = MinMaxScaler()            \n",
    "            \n",
    "    def fit(self, graphs):\n",
    "        ''' Fit the scalers on an array of x and ea features\n",
    "        '''\n",
    "        x, y, ea = from_graphs_to_pandas(graphs)\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            self.scalers[feat] = self.scalers[feat].fit(x[:,ix].reshape(-1,1))\n",
    "        self.scalers[self.output] = self.scalers[self.output].fit(y.reshape(-1,1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            self.scalers[feat] = self.scalers[feat].fit(ea[:,ix].reshape(-1,1))        \n",
    "        return self\n",
    "\n",
    "    def transform(self, graph):\n",
    "        ''' Transform graph based on normalizer\n",
    "        '''\n",
    "        graph = graph.clone()\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            temp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "            graph.x[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            temp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "            graph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].transform(temp).reshape(-1))\n",
    "        graph.y = torch.tensor(self.scalers[self.output].transform(graph.y.numpy().reshape(-1,1)).reshape(-1))                   \n",
    "        return graph\n",
    "\n",
    "    def inverse_transform(self, graph):\n",
    "        ''' Perform inverse transformation to return original features\n",
    "        '''\n",
    "        graph = graph.clone()\n",
    "        for ix, feat in enumerate(self.x_feat_names):\n",
    "            temp = graph.x[:,ix].numpy().reshape(-1,1)\n",
    "            graph.x[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "        for ix, feat in enumerate(self.ea_feat_names):\n",
    "            temp = graph.edge_attr[:,ix].numpy().reshape(-1,1)\n",
    "            graph.edge_attr[:,ix] = torch.tensor(self.scalers[feat].inverse_transform(temp).reshape(-1))\n",
    "        graph.y = torch.tensor(self.scalers[self.output].inverse_transform(graph.y.numpy().reshape(-1,1)).reshape(-1))                   \n",
    "        return graph\n",
    "            \n",
    "    def transform_array(self,z,feat_name):\n",
    "        '''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "        return torch.tensor(self.scalers[feat_name].transform(z).reshape(-1))\n",
    "        \n",
    "    def inverse_transform_array(self,z,feat_name):\n",
    "        '''\n",
    "            This is for MLP dataset; it can be done better (the entire thing, from raw data to datasets)\n",
    "        '''\n",
    "        return torch.tensor(self.scalers[feat_name].inverse_transform(z).reshape(-1))\n",
    "\n",
    "def from_graphs_to_pandas(graphs, l_x=3, l_ea=3):\n",
    "    x = []\n",
    "    y = []\n",
    "    ea = []\n",
    "    for i, graph in enumerate(graphs):\n",
    "        x.append(graph.x.numpy())\n",
    "        y.append(graph.y.reshape(-1,1).numpy())\n",
    "        ea.append(graph.edge_attr.numpy())     \n",
    "    return np.concatenate(x,axis=0),np.concatenate(y,axis=0),np.concatenate(ea,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1909119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# constant indexes for node and edge features\n",
    "ELEVATION_INDEX = 0\n",
    "BASEDEMAND_INDEX = 1\n",
    "BASEHEAD_INDEX = 2\n",
    "DIAMETER_INDEX = 0\n",
    "LENGTH_INDEX = 1\n",
    "ROUGHNESS_INDEX = 2\n",
    "\n",
    "def load_raw_dataset(wdn_name, data_folder):\n",
    "    '''\n",
    "    Load tra/val/data for a water distribution network datasets\n",
    "    -------\n",
    "    wdn_name : string\n",
    "        prefix of pickle files to open\n",
    "    data_folder : string\n",
    "        path to datasets\n",
    "    '''\n",
    "\n",
    "    data_tra = pickle.load(open(f'{data_folder}/train/{wdn_name}.p', \"rb\"))\n",
    "    data_val = pickle.load(open(f'{data_folder}/valid/{wdn_name}.p', \"rb\"))\n",
    "    data_tst = pickle.load(open(f'{data_folder}/test/{wdn_name}.p', \"rb\"))\n",
    "\n",
    "    return data_tra, data_val, data_tst\n",
    "\n",
    "def create_dataset(database, normalizer=None, HW_rough_minmax=[60, 150],add_virtual_reservoirs=False, output='pressure'):\n",
    "    '''\n",
    "    Creates working datasets dataset from the pickle databases\t\n",
    "    ------    \n",
    "    database : list\n",
    "        each element in the list is a pickle file containing Data objects\n",
    "    normalization: dict\n",
    "        normalize the dataset using mean and std\n",
    "    '''\n",
    "    # Roughness info (Hazen-Williams) / TODO: remove the hard_coding\n",
    "    minR = HW_rough_minmax[0]\n",
    "    maxR = HW_rough_minmax[1]\n",
    "\n",
    "    graphs = []\n",
    "\n",
    "    for i in database:\n",
    "        graph = torch_geometric.data.Data()\n",
    "\n",
    "        # Node attributes\n",
    "        graph.x = torch.stack((i.elevation+i.base_head, i.base_demand, i.type_1H), dim=1).float()\n",
    "\n",
    "        # Position and ID\n",
    "        graph.ID = i.ID\n",
    "\n",
    "        # Edge index (Adjacency matrix)\n",
    "        graph.edge_index = i.edge_index\n",
    "\n",
    "        # Edge attributes\n",
    "        diameter = i.diameter\n",
    "        length = i.length\n",
    "        roughness = i.roughness\n",
    "        graph.edge_attr = torch.stack((diameter, length, roughness), dim=1).float()\n",
    "\n",
    "        # Graph output (head)\n",
    "        if output == 'head':\n",
    "            raise ValueError('Not yet implemented')\n",
    "        else:\n",
    "            graph.y = i.pressure[i.type_1H == 0].reshape(-1, 1)\n",
    "\n",
    "        \n",
    "        # normalization\n",
    "        if normalizer is not None:\n",
    "            graph = normalizer.transform(graph)\n",
    "\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def create_dataset_MLP_from_graphs(graphs, features=['diameter', 'base_demand', 'roughness'],no_res_out=True):\n",
    "    '''\n",
    "    TO DO\n",
    "    '''\n",
    "\n",
    "    # index edges to avoid duplicates: this considers all graphs to be UNDIRECTED!\n",
    "    ix_edge = graphs[0].edge_index.numpy().T\n",
    "    ix_edge = (ix_edge[:, 0] < ix_edge[:, 1])\n",
    "    \n",
    "    # position of reservoirs\n",
    "    ix_res = graphs[0].x[:,BASEHEAD_INDEX].numpy()>0\n",
    "\n",
    "    for ix_feat, feature in enumerate(features):\n",
    "        for ix_item, item in enumerate(graphs):\n",
    "            if feature == 'diameter':                \n",
    "                x_ = item.edge_attr[ix_edge,DIAMETER_INDEX]\n",
    "            elif feature == 'roughness':\n",
    "                x_ = item.edge_attr[ix_edge,ROUGHNESS_INDEX]\n",
    "            elif feature == 'base_demand':\n",
    "                # remove reservoirs\n",
    "                x_ = item.x[~ix_res,BASEDEMAND_INDEX]\n",
    "            else:\n",
    "                raise ValueError(f'Feature {feature} not supported.')\n",
    "            if ix_item == 0:\n",
    "                x = x_\n",
    "            else:\n",
    "                x = torch.cat((x, x_), dim=0)\n",
    "        if ix_feat == 0:\n",
    "            X = x.reshape(len(graphs), -1)\n",
    "        else:\n",
    "            X = torch.cat((X, x.reshape(len(graphs), -1)), dim=1)\n",
    "\n",
    "    for ix_item, item in enumerate(graphs):\n",
    "        # remove reservoirs from y as well\n",
    "        if ix_item == 0:\n",
    "            if no_res_out == True:\n",
    "                y = item.y\n",
    "            else:\n",
    "                y = item.y[~ix_res]\n",
    "        else:\n",
    "            if no_res_out == True:\n",
    "                y = torch.cat((y, item.y), dim=0)\n",
    "            else:\n",
    "                y = torch.cat((y, item.y[~ix_res]), dim=0)\n",
    "    y = y.reshape(len(graphs), -1)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(X, y)    \n",
    "    # return X,y,ix_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdda9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch_geometric.nn.models\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout\n",
    "from torch_geometric.nn import ChebConv, MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn.inits import reset\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class NNConvEmbed(MessagePassing):\n",
    "    def __init__(self, x_num, ea_num, emb_channels, aggr, dropout_rate=0):\n",
    "        super(NNConvEmbed, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.x_num = x_num\n",
    "        self.ea_num = ea_num\n",
    "        self.emb_channels = emb_channels\n",
    "        self.nn = Sequential(Linear(2 * x_num + ea_num, emb_channels), Dropout(p=dropout_rate), ReLU())\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "        return self.nn(z)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(aggr=\"{}\", nn={})'.format(self.__class__.__name__, self.aggr, self.nn)\n",
    "\n",
    "\n",
    "class GNN_ChebConv(nn.Module):\n",
    "    def __init__(self, hid_channels, edge_features, node_features, edge_channels=32, dropout_rate=0, CC_K=2,\n",
    "                 emb_aggr='max', depth=2, normalize=True):\n",
    "        super(GNN_ChebConv, self).__init__()\n",
    "        self.hid_channels = hid_channels\n",
    "        self.dropout = dropout_rate\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # embedding of node/edge features with NN\n",
    "        self.embedding = NNConvEmbed(node_features, edge_features, edge_channels, aggr=emb_aggr)\n",
    "\n",
    "        # CB convolutions (with normalization)\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                self.convs.append(ChebConv(edge_channels, hid_channels, CC_K, normalization='sym'))\n",
    "            else:\n",
    "                self.convs.append(ChebConv(hid_channels, hid_channels, CC_K, normalization='sym'))\n",
    "\n",
    "        # output layer (so far only a 1 layer MLP, make more?)\n",
    "        if depth == 0:\n",
    "            self.lin = Linear(edge_channels, 1)\n",
    "        else:\n",
    "            self.lin = Linear(hid_channels, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        # retrieve model device (for LayerNorm to work)\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        # 1. Pre-process data (nodes and edges) with MLP\n",
    "        x = self.embedding(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        # 2. Do convolutions\n",
    "        for i in range(len(self.convs)):\n",
    "            x = self.convs[i](x=x, edge_index=edge_index)\n",
    "            if self.normalize:\n",
    "                x = nn.LayerNorm(self.hid_channels, eps=1e-5, device=device)(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = nn.ReLU()(x)\n",
    "\n",
    "        # 3. Output\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Mask over storage nodes (which have pressure=0)\n",
    "        x = x[data.x[:,2]<1,0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8caefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hid_channels, num_inputs, num_outputs, num_layers=2, dropout_rate=0):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.hid_channels = hid_channels\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        layers = [Linear(num_inputs, hid_channels),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Dropout(self.dropout_rate)]\n",
    "        \n",
    "        for l in range(num_layers-1):\n",
    "            layers += [Linear(hid_channels, hid_channels),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Dropout(self.dropout_rate)]\n",
    "            \n",
    "        # layers += [Linear(hid_channels, num_outputs),nn.Sigmoid()]\n",
    "        layers += [Linear(hid_channels, num_outputs)]\n",
    "        \n",
    "        self.main = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.main(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b220460",
   "metadata": {},
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3dc9a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with FOS, network 1 of 6\n",
      "i=0; skipping\n",
      "i=1; skipping\n",
      "i=2; skipping\n",
      "i=3; skipping\n",
      "i=4; skipping\n",
      "i=5; skipping\n",
      "i=6; skipping\n",
      "i=7; skipping\n",
      "i=8; skipping\n",
      "i=9; skipping\n",
      "i=10; skipping\n",
      "i=11; skipping\n",
      "i=12; skipping\n",
      "i=13; skipping\n",
      "i=14; skipping\n",
      "i=15; skipping\n",
      "i=16; skipping\n",
      "i=17; skipping\n",
      "GNN_ChebConv: training combination 21 of 24\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/bulat/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13655/2832782963.py\", line 85, in <module>\n",
      "    model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
      "  File \"/home/bulat/PycharmProjects/gnn_wds_deploy/training/train.py\", line 117, in training\n",
      "    train_loss = train_epoch(model, train_loader, optimizer, alpha=alpha, normalization=normalization, device=device)\n",
      "  File \"/home/bulat/PycharmProjects/gnn_wds_deploy/training/train.py\", line 47, in train_epoch\n",
      "    preds = model(batch)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_13655/2584101711.py\", line 80, in forward\n",
      "    x = self.convs[i](x=x, edge_index=edge_index)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/cheb_conv.py\", line 178, in forward\n",
      "    Tx_2 = self.propagate(edge_index, x=Tx_1, norm=norm, size=None)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\", line 432, in propagate\n",
      "    msg_kwargs = self.inspector.distribute('message', coll_dict)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/utils/inspector.py\", line 60, in distribute\n",
      "    out[key] = data\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bulat/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bulat/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bulat/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bulat/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/inspect.py\", line 721, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/posixpath.py\", line 375, in abspath\n",
      "    if not isabs(path):\n",
      "  File \"/home/bulat/miniconda3/envs/graphenv38/lib/python3.8/posixpath.py\", line 64, in isabs\n",
      "    return s.startswith(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_13655/2832782963.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     84\u001B[0m             \u001B[0;31m# training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m             model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n\u001B[0m\u001B[1;32m     86\u001B[0m                                                                    \u001B[0mn_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreport_freq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/gnn_wds_deploy/training/train.py\u001B[0m in \u001B[0;36mtraining\u001B[0;34m(model, optimizer, train_loader, val_loader, n_epochs, patience, report_freq, alpha, lr_rate, lr_epoch, normalization, device)\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;31m# Model training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m         \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalization\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnormalization\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/gnn_wds_deploy/training/train.py\u001B[0m in \u001B[0;36mtrain_epoch\u001B[0;34m(model, loader, optimizer, alpha, normalization, device)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0;31m# Model prediction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m             \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_13655/2584101711.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medge_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0medge_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/cheb_conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001B[0m\n\u001B[1;32m    177\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlin\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 178\u001B[0;31m             \u001B[0mTx_2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpropagate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0medge_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mTx_1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    179\u001B[0m             \u001B[0mTx_2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2.\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mTx_2\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mTx_0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001B[0m in \u001B[0;36mpropagate\u001B[0;34m(self, edge_index, size, **kwargs)\u001B[0m\n\u001B[1;32m    431\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 432\u001B[0;31m                 \u001B[0mmsg_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'message'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoll_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    433\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_message_forward_pre_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/graphenv38/lib/python3.8/site-packages/torch_geometric/nn/conv/utils/inspector.py\u001B[0m in \u001B[0;36mdistribute\u001B[0;34m(self, func_name, kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m             \u001B[0mout\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2063\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2064\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2065\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2064\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2065\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2066\u001B[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[1;32m   2067\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[1;32m   2068\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1365\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1367\u001B[0;31m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[1;32m   1369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1265\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1267\u001B[0;31m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1269\u001B[0m             )\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1122\u001B[0m         \u001B[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1124\u001B[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[1;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    380\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for ix_wdn, wdn in enumerate(all_wdn_names):\n",
    "    print(f'\\nWorking with {wdn}, network {ix_wdn+1} of {len(all_wdn_names)}')\n",
    "    \n",
    "    # retrieve wntr data\n",
    "    tra_database, val_database, tst_database = load_raw_dataset(wdn, data_folder)\n",
    "    # reduce training data    \n",
    "    # tra_database = tra_database[:int(len(tra_database)*cfg['tra_prc'])]\n",
    "    if cfg['tra_num'] < len(tra_database):\n",
    "        tra_database = tra_database[:cfg['tra_num']]\n",
    "        \n",
    "    # remove PES anomaly\n",
    "    if wdn == 'PES':\n",
    "        if len(tra_database)>4468:\n",
    "            del tra_database[4468]\n",
    "            print('Removed PES anomaly')\n",
    "            print('Check',tra_database[4468].pressure.mean())\n",
    "    \n",
    "    # get GRAPH datasets    # later on we should change this and use normal scalers from scikit\n",
    "    tra_dataset = create_dataset(tra_database)\n",
    "    gn = GraphNormalizer()\n",
    "    gn = gn.fit(tra_dataset)\n",
    "    tra_dataset = create_dataset(tra_database, normalizer=gn)\n",
    "    val_dataset = create_dataset(val_database, normalizer=gn)\n",
    "    tst_dataset = create_dataset(tst_database, normalizer=gn)\n",
    "    node_size, edge_size = tra_dataset[0].x.size(-1), tra_dataset[0].edge_attr.size(-1)  \n",
    "    # number of nodes\n",
    "    # n_nodes=tra_dataset[0].x.shape[0]\n",
    "    n_nodes=(1-tra_database[0].type_1H).numpy().sum() # remove reservoirs\n",
    "    \n",
    "    # loop through different algorithms\n",
    "    for algorithm in cfg['algorithms']:\n",
    "        hyperParams = cfg['hyperParams'][algorithm]\n",
    "        all_combinations = ParameterGrid(hyperParams)\n",
    "                        \n",
    "        # dataloader\n",
    "        if algorithm == 'MLP':\n",
    "            # transform dataset for MLP / ad-hoc addition to remove asap                \n",
    "            tra_loader = torch.utils.data.DataLoader(create_dataset_MLP_from_graphs(tra_dataset), \n",
    "                                                     batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "            val_loader = torch.utils.data.DataLoader(create_dataset_MLP_from_graphs(val_dataset), \n",
    "                                                     batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "            tst_loader = torch.utils.data.DataLoader(create_dataset_MLP_from_graphs(tst_dataset), \n",
    "                                                     batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "        else:\n",
    "            # it's a GNN\n",
    "            tra_loader = torch_geometric.loader.DataLoader(tra_dataset, batch_size=batch_size, \n",
    "                                                           shuffle=True, pin_memory=True)\n",
    "            val_loader = torch_geometric.loader.DataLoader(val_dataset, batch_size=batch_size, \n",
    "                                                           shuffle=False, pin_memory=True)                \n",
    "            tst_loader = torch_geometric.loader.DataLoader(tst_dataset, batch_size=batch_size, \n",
    "                                                           shuffle=False, pin_memory=True)\n",
    "\n",
    "        \n",
    "        # create results dataframe\n",
    "        results_df = pd.DataFrame(list(all_combinations))\n",
    "        results_df = pd.concat([results_df,\n",
    "                                pd.DataFrame(index=np.arange(len(all_combinations)), \n",
    "                                          columns=list(res_columns))],axis=1)        \n",
    "        \n",
    "        for i, combination in enumerate(all_combinations):\n",
    "            if i<18:\n",
    "                print(f'i={i}; skipping')\n",
    "                continue\n",
    "            # TO DO: This hardcoded if/else below ain't good --> let's change it\n",
    "            if algorithm == 'MLP':\n",
    "                for temp in tra_loader:\n",
    "                    break\n",
    "                combination['num_inputs']  = temp[0].shape[1]  # all this ad-hoc stuff must be removed\n",
    "                combination['num_outputs'] = temp[1].shape[1]  # (e.g., these lines are needed to instantiate MLP properly)            \n",
    "            else:\n",
    "                combination['edge_features']=edge_size # all this ad-hoc stuff must be removed\n",
    "                combination['node_features']=node_size # (e.g., these two lines are needed to instantiate GNN properly)\n",
    "            \n",
    "            print(f'{algorithm}: training combination {i+1} of {len(all_combinations)}\\t',end='\\r',)    \n",
    "\n",
    "            # model creation\n",
    "            model = getattr(sys.modules[__name__], algorithm)(**combination).to(device)                        \n",
    "            total_parameters = sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            # model optimizer\n",
    "            optimizer = optim.Adam(params=model.parameters(), **cfg['adamParams'])            \n",
    "\n",
    "            # training\n",
    "            model, tra_losses, val_losses, elapsed_time = training(model, optimizer, tra_loader, val_loader,\n",
    "                                                                   n_epochs=num_epochs, patience=10, report_freq=0, \n",
    "                                                                   alpha=alpha, lr_rate=5, lr_epoch=50, \n",
    "                                                                   normalization=None)\n",
    "            # store training history and model\n",
    "            pd.DataFrame(data = np.array([tra_losses, val_losses]).T).to_csv(\n",
    "                f'{results_folder}/{wdn}/{algorithm}/hist/{i}.csv')\n",
    "            torch.save(model, f'{results_folder}/{wdn}/{algorithm}/models/{i}.csv')\n",
    "                         \n",
    "            # compute and store predictions, compute r2 scores        \n",
    "            losses = {}\n",
    "            r2_scores = {}            \n",
    "            for split, loader in zip(['training','validation','testing'],[tra_loader,val_loader,tst_loader]):\n",
    "                losses[split], pred, real = testing(model, loader)\n",
    "                r2_scores[split] = r2_score(real, pred)\n",
    "                if i == 0:\n",
    "                    pd.DataFrame(data=real.reshape(-1,n_nodes)).to_csv(\n",
    "                        f'{results_folder}/{wdn}/{algorithm}/pred/{split}/real.csv') # save real obs\n",
    "                pd.DataFrame(data=pred.reshape(-1,n_nodes)).to_csv(\n",
    "                    f'{results_folder}/{wdn}/{algorithm}/pred/{split}/{i}.csv')\n",
    "                \n",
    "            # store results\n",
    "            results_df.loc[i,res_columns] = (losses['training'], losses['validation'], losses['testing'], \n",
    "                                             r2_scores['training'], r2_scores['validation'], r2_scores['testing'], \n",
    "                                             total_parameters, elapsed_time, num_epochs)\n",
    "        # save graphnormalizer\n",
    "        with open(f'{results_folder}/{wdn}/{algorithm}/gn.pickle', 'wb') as handle:\n",
    "            pickle.dump(gn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results_df.to_csv(f'{results_folder}/{wdn}/{algorithm}/results.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b178c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}